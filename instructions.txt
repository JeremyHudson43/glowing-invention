### ðŸ” Main Agent Loop Overview

Each iteration follows this loop:

> **Step 1 â†’ Step 2 â†’ Step 3 â†’ Step 4 â†’ Step 5 â†’ Return to Step 1**

---

## âœ… STEP 1: Load and Understand Contest Rules

**Path**:
`F:\cmi-detect-behavior-with-sensor-data\contest_details.txt`

* Read the full contents of `contest_details.txt`.
* Refresh memory each loop â€” contest constraints, metric goals (F1), sensor modality breakdown, etc.
* Extract or update constraints for inference-time and training-time modalities (e.g. 50% IMU-only test samples).

---

## âœ… STEP 2: Load & Analyze Training Data

**Files**:

* `F:\cmi-detect-behavior-with-sensor-data\train.csv`
* `F:\cmi-detect-behavior-with-sensor-data\train_demographics.csv`

**Goals**:

* Determine input format (columns, datatypes, sensor types, missingness).
* Understand label distributions and class imbalance.
* Determine which features correspond to IMU only (acc, gyro), proximity, temp, etc.
* Detect patient-level grouping or ID features (if needed for GroupKFold or similar).
* Cache metadata that supports conditional modeling decisions (e.g. modality presence).

---

## âœ… STEP 3: Propose One Minimal, Targeted Model Modification

**Only one of the following per iteration**:

* ðŸ“ˆ **Data Augmentation** (e.g. jitter, masking, cutmix, mixup, modality dropout)
* ðŸ§  **Model Architecture Change** (e.g. replace Transformer block, add depthwise conv stem)
* âš–ï¸ **Loss Function Change** (e.g. focal loss, class-weighting)
* ðŸ” **Normalization or Preprocessing Update** (e.g. per-user z-score, robust scaler)
* ðŸ§ª **Input Handling or Sensor Dropout** (simulate IMU-only test cases during training)

> **âš ï¸ DO NOT:**
>
> * Rewrite the full architecture
> * Make multiple changes at once
> * Add helper test scripts
> * Use TensorFlow (PyTorch-only)

---

## âœ… STEP 4: Save & Run Code Attempt

### ðŸ”§ Implementation

* Implement the change in a single script.
* Place file at:
  `F:\cmi-detect-behavior-with-sensor-data\attempts\attempt_<x>__<CHANGE_TYPE>_<pass/fail>.py`

> ðŸ”¤ Use underscores exactly like this: `attempt_1__mixup_strength_pass.py`

### ðŸ“‹ Evaluation

* Run the script on your machine.
* Evaluate whether it improves validation **F1 score**.
* Label the script as `pass` if F1 increases, `fail` if it does not.

---

## âœ… STEP 5: Log the Attempt

### ðŸ“ Log file path:

`F:\cmi-detect-behavior-with-sensor-data\attempts\attempt_logs`

### ðŸ“ Log Format:

Create or append a plain text log entry like this:

```
Attempt 1
Change Type: mixup_strength
Description: Increased mixup alpha from 0.2 â†’ 0.6 to test if stronger augmentation improves generalization.
Result: FAIL
Validation F1 dropped slightly, possibly due to overmixing in low-data classes.
```

---

## ðŸ” Return to Step 1

Re-read contest rules and data files again before the next attempt to refresh and discover new constraints or ideas.


"""
CMI â€“ Detect Behavior with Sensor Data (BFRB)
Endâ€‘toâ€‘end notebook for Kaggle: trains + saves + launches evaluation server.
Key improvements over baseline:
â€¢ Perâ€‘sequence zâ€‘score + jerk (Î”acc, Î”gyro)
â€¢ Global stats branch (min/max/mean/std/range)
â€¢ 2â€‘layer Transformer encoder backbone
This script runs within 9â€‘h GPU limit and yields a valid submission in one go.
"""

# ===============================
# 1. Imports & paths
# ===============================
import os, gc, warnings
import numpy as np
import pandas as pd
import polars as pl
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.utils.rnn import pad_sequence
import kaggle_evaluation.cmi_inference_server

warnings.filterwarnings("ignore")

DATA_DIR = "/kaggle/input/cmi-detect-behavior-with-sensor-data"
TRAIN_CSV = os.path.join(DATA_DIR, "train.csv")
TEST_CSV = os.path.join(DATA_DIR, "test.csv")
TEST_DEM = os.path.join(DATA_DIR, "test_demographics.csv")

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===============================
# 2. Load & encode labels
# ===============================
print("Loading train â€¦")
df = pd.read_csv(TRAIN_CSV)
print(f"Rows: {len(df):,}")

le = LabelEncoder()
df["gesture"] = le.fit_transform(df["gesture"].astype(str))
np.save("gesture_classes.npy", le.classes_)
NUM_CLASSES = len(le.classes_)

# ===============================
# 3. Feature cols
# ===============================
EXCL = {"gesture", "sequence_type", "behavior", "orientation",
        "row_id", "subject", "phase", "sequence_id", "sequence_counter"}
THERM_TOF = [c for c in df.columns if c.startswith("thm_") or c.startswith("tof_")]
EXCL.update(THERM_TOF)  # IMUâ€‘only baseline
FEATURE_COLS = [c for c in df.columns if c not in EXCL]
ACC_COLS = [c for c in FEATURE_COLS if c.startswith("acc_")]
GYR_COLS = [c for c in FEATURE_COLS if c.startswith("rot_")]
np.save("feature_cols.npy", np.array(FEATURE_COLS))
print(f"Using {len(FEATURE_COLS)} base features (IMU).")

# ===============================
# 4. Preâ€‘processing helpers
# ===============================

def jerk(df_sub):
    d = df_sub[ACC_COLS + GYR_COLS].diff().fillna(0)
    d.columns = [f"d_{c}" for c in d.columns]
    return d


def zscore(mat):
    scaler = StandardScaler()
    return scaler.fit_transform(mat)


def stats(mat):
    mins, maxs = mat.min(0), mat.max(0)
    means, stds = mat.mean(0), mat.std(0)
    rng = maxs - mins
    return np.concatenate([mins, maxs, means, stds, rng]).astype("float32")


def preprocess_seq(df_seq):
    seq = df_seq.sort_values("sequence_counter")
    base = seq[FEATURE_COLS].ffill().bfill().fillna(0)
    full = pd.concat([base, jerk(base)], axis=1)
    arr = zscore(full)
    return arr.astype("float32"), stats(arr)


# PyTorch pad_sequences equivalent
def pad_sequences_torch(sequences, maxlen, padding='post', truncating='post', value=0.0):
    padded = []
    for seq in sequences:
        seq_tensor = torch.tensor(seq, dtype=torch.float32)
        if len(seq_tensor) > maxlen:
            if truncating == 'post':
                seq_tensor = seq_tensor[:maxlen]
            else:
                seq_tensor = seq_tensor[-maxlen:]
        
        if len(seq_tensor) < maxlen:
            pad_size = maxlen - len(seq_tensor)
            if padding == 'post':
                seq_tensor = F.pad(seq_tensor, (0, 0, 0, pad_size), value=value)
            else:
                seq_tensor = F.pad(seq_tensor, (0, 0, pad_size, 0), value=value)
        
        padded.append(seq_tensor)
    
    return torch.stack(padded)

# ===============================
# 5. Build tensors
# ===============================
seqs, gl_stats, labels, lengths = [], [], [], []
for sid, grp in df.groupby("sequence_id"):
    a, s = preprocess_seq(grp)
    seqs.append(a); gl_stats.append(s); labels.append(grp["gesture"].iloc[0])
    lengths.append(len(a))
PAD_LEN = int(np.percentile(lengths, 90))
np.save("sequence_maxlen.npy", PAD_LEN)

seqs = pad_sequences_torch(seqs, maxlen=PAD_LEN, padding="post", truncating="post")
stat_arr = torch.tensor(np.stack(gl_stats), dtype=torch.float32)
Y = torch.tensor(labels, dtype=torch.long)

# Train-val split
indices = np.arange(len(seqs))
train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=labels)

X_seq_tr = seqs[train_idx]
X_seq_val = seqs[val_idx]
X_stat_tr = stat_arr[train_idx]
X_stat_val = stat_arr[val_idx]
y_tr = Y[train_idx]
y_val = Y[val_idx]

DELTA_F = seqs.shape[2]
STAT_F = stat_arr.shape[1]
print(f"Pad length {PAD_LEN}, sequence features {DELTA_F}, stats {STAT_F}")

# ===============================
# 6. Transformer model
# ===============================

class TransformerBlock(nn.Module):
    def __init__(self, d_model=128, n_heads=4, ff=256, drop=0.1):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=drop, batch_first=True)
        self.ln1 = nn.LayerNorm(d_model, eps=1e-6)
        self.ln2 = nn.LayerNorm(d_model, eps=1e-6)
        self.ff = nn.Sequential(
            nn.Linear(d_model, ff),
            nn.ReLU(),
            nn.Linear(ff, d_model)
        )
        self.dropout = nn.Dropout(drop)
    
    def forward(self, x, mask=None):
        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)
        x = self.ln1(x + self.dropout(attn_out))
        ff_out = self.ff(x)
        x = self.ln2(x + self.dropout(ff_out))
        return x


class BFRBTransformer(nn.Module):
    def __init__(self, seq_len, delta_f, stat_f, num_classes, d_model=128):
        super().__init__()
        self.proj = nn.Linear(delta_f, d_model)
        self.transformer1 = TransformerBlock(d_model)
        self.transformer2 = TransformerBlock(d_model)
        
        self.stat_feat = nn.Sequential(
            nn.Linear(stat_f, 128),
            nn.ReLU()
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, seq_input, stat_input):
        # Create padding mask (True where padded)
        mask = (seq_input == 0).all(dim=-1)
        
        # Project sequence features
        x = self.proj(seq_input)
        
        # Apply transformer blocks
        x = self.transformer1(x, mask)
        x = self.transformer2(x, mask)
        
        # Global average pooling (masked)
        mask_expanded = mask.unsqueeze(-1).expand_as(x)
        x.masked_fill_(mask_expanded, 0)
        seq_feat = x.sum(dim=1) / (~mask).sum(dim=1, keepdim=True).float()
        
        # Process stats
        stat_feat = self.stat_feat(stat_input)
        
        # Concatenate and classify
        combined = torch.cat([seq_feat, stat_feat], dim=1)
        return self.classifier(combined)


model = BFRBTransformer(PAD_LEN, DELTA_F, STAT_F, NUM_CLASSES).to(device)
print(model)

# ===============================
# 7. Train
# ===============================
print("Training â€¦")

# Create datasets and dataloaders
train_dataset = TensorDataset(X_seq_tr, X_stat_tr, y_tr)
val_dataset = TensorDataset(X_seq_val, X_stat_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

# Loss and optimizer
criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)

# Training loop
best_val_loss = float('inf')
patience_counter = 0
best_model_state = None

for epoch in range(60):
    # Training
    model.train()
    train_loss = 0
    train_correct = 0
    train_total = 0
    
    for seq_batch, stat_batch, label_batch in train_loader:
        seq_batch = seq_batch.to(device)
        stat_batch = stat_batch.to(device)
        label_batch = label_batch.to(device)
        
        optimizer.zero_grad()
        outputs = model(seq_batch, stat_batch)
        loss = criterion(outputs, label_batch)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = outputs.max(1)
        train_total += label_batch.size(0)
        train_correct += predicted.eq(label_batch).sum().item()
    
    # Validation
    model.eval()
    val_loss = 0
    val_correct = 0
    val_total = 0
    
    with torch.no_grad():
        for seq_batch, stat_batch, label_batch in val_loader:
            seq_batch = seq_batch.to(device)
            stat_batch = stat_batch.to(device)
            label_batch = label_batch.to(device)
            
            outputs = model(seq_batch, stat_batch)
            loss = criterion(outputs, label_batch)
            
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            val_total += label_batch.size(0)
            val_correct += predicted.eq(label_batch).sum().item()
    
    avg_val_loss = val_loss / len(val_loader)
    train_acc = 100. * train_correct / train_total
    val_acc = 100. * val_correct / val_total
    
    print(f'Epoch {epoch+1}/60 - loss: {train_loss/len(train_loader):.4f} - '
          f'accuracy: {train_acc:.2f}% - val_loss: {avg_val_loss:.4f} - '
          f'val_accuracy: {val_acc:.2f}%')
    
    scheduler.step(avg_val_loss)
    
    # Early stopping
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        best_model_state = model.state_dict().copy()
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= 8:
            print("Early stopping!")
            break

# Restore best weights
model.load_state_dict(best_model_state)

# Save model
torch.save({
    'model_state_dict': model.state_dict(),
    'pad_len': PAD_LEN,
    'delta_f': DELTA_F,
    'stat_f': STAT_F,
    'num_classes': NUM_CLASSES
}, 'bfrb_transformer.pth')

# ===============================
# 8. Inference utils
# ===============================
CLASS_NAMES = np.load("gesture_classes.npy", allow_pickle=True)
FEATURE_COLS = np.load("feature_cols.npy", allow_pickle=True).tolist()
ACC_COLS = [c for c in FEATURE_COLS if c.startswith("acc_")]
GYR_COLS = [c for c in FEATURE_COLS if c.startswith("rot_")]

# Load model for inference
checkpoint = torch.load('bfrb_transformer.pth', map_location=device)
_loaded = BFRBTransformer(
    checkpoint['pad_len'], 
    checkpoint['delta_f'], 
    checkpoint['stat_f'], 
    checkpoint['num_classes']
).to(device)
_loaded.load_state_dict(checkpoint['model_state_dict'])
_loaded.eval()


def preprocess_for_inf(df_seq):
    s_arr, g_stat = preprocess_seq(df_seq)
    s_pad = pad_sequences_torch([s_arr], maxlen=PAD_LEN, padding="post", truncating="post")
    return s_pad, torch.tensor(g_stat[np.newaxis, :], dtype=torch.float32)


def predict(sequence: pl.DataFrame, demographics: pl.DataFrame):
    s_pad, g_stat = preprocess_for_inf(sequence.to_pandas())
    s_pad = s_pad.to(device)
    g_stat = g_stat.to(device)
    
    with torch.no_grad():
        outputs = _loaded(s_pad, g_stat)
        probs = F.softmax(outputs, dim=1)
        pred_idx = torch.argmax(probs, dim=1).cpu().numpy()[0]
    
    return CLASS_NAMES[int(pred_idx)]

# ===============================
# 9. Launch server
# ===============================
serv = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)
if os.getenv("KAGGLE_IS_COMPETITION_RERUN"):
    serv.serve()
else:
    serv.run_local_gateway(data_paths=(TEST_CSV, TEST_DEM))

print("Done â€“ submission ready.")